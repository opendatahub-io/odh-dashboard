projectDescription: 'Gen AI Test Project for model connections'
connectionName: 'llama-3.2-1b-instruct'
connectionDescription: 'URI connection for Llama 3.2 1B Instruct model'
connectionType: 'URI - v1'
connectionURI: 'oci://quay.io/redhat-ai-services/modelcar-catalog:llama-3.2-1b-instruct'
modelDeploymentName: 'llama-3.2-1b-instruct'
inferenceServiceName: 'llama-32-1b-instruct'
modelType: 'Generative AI model (Example, LLM)'
servingRuntime: 'vLLM CPU (amd64 - EXPERIMENTAL)'
testMessage: 'Hello, this is a test message.'
# Hardware Profile configuration
hardwareProfileResourceYamlPath: 'resources/hardwareProfile/gen_ai_hardware_profile.yaml'
hardwareProfileName: 'cypress-gen-ai-hardware-profile'
hardwareProfileDeploymentSize: 'cypress-gen-ai-hardware-profile Compatible CPU: Request = 1 Cores; Limit = 1 Cores; Memory: Request = 8 GiB; Limit = 8 GiB'
# Playground resources
configMapName: 'llama-stack-config'
playgroundServiceName: 'lsd-genai-playground-service'

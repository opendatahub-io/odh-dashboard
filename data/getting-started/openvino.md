# OpenVINO Toolkit

[OpenVINO Toolkit](https://software.intel.com/content/www/us/en/develop/tools/openvino-toolkit.html) - Deploy High-Performance Deep Learning Inference

## Your AI Inferencing Apps...Now Faster
Develop applications and solutions that use deep learning intelligence with the Intel® Distribution of OpenVINO™ toolkit. Based on convolutional neural networks (CNN), the toolkit extends workloads across Intel® hardware (including accelerators) and maximizes performance.

Features include:

* Enables deep learning inference from edge to cloud
* Accelerates AI workloads, including computer vision, audio, speech, language, and recommendation systems
* Supports heterogeneous execution across Intel® architecture and AI accelerators - CPU, iGPU, Intel® Movidius™ Vision Processing Unit (VPU), FPGA, and Intel® Gaussian & Neural Accelerator (Intel® GNA) - using a common API
* Speeds up time to market via a library of functions and preoptimized kernels
* Includes optimized calls for OpenCV, OpenCL™ kernels, and other industry tools and libraries

## Installing OpenVINO Toolkit

OpenVINO libraries are provided through PyPi, and can be installed from many existing notebook container images. Intel provides a number of certified container images that have OpenVINO Toolkit components pre-installed, and ready to use.

## Installing OpenVINO Operator

OpenVINO Operator has a Red Hat marketplace listing.

### Subscribe to the operator on Marketplace
- [https://marketplace.redhat.com/en-us/products/openvino](https://marketplace.redhat.com/en-us/products/openvino)
### Install the operator and validate
- [https://marketplace.redhat.com/en-us/documentation/operators](https://marketplace.redhat.com/en-us/documentation/operators)

name: Cypress e2e Test

# This workflow currently executes on the ODH (Open Data Hub) dash-e2e cluster.
# The test-variables.yml downloaded from GitLab contains RHOAI namespaces by default,
# which are overridden with ODH namespaces from the ODH_NAMESPACES GitHub secret.
#
# This workflow runs on pull_request events and waits for test.yml to complete successfully.
# It uses concurrency control to ensure only the latest E2E run executes for each PR/branch.
# This approach ensures E2E checks appear in the PR's checks section.
on:
  # Testing: Use pull_request to test workflow changes from PR branch
  # TODO: After merging, switch to pull_request_target for fork PR support
  pull_request:
    types: [opened, synchronize, reopened]
  # Allow manual trigger for testing/debugging
  workflow_dispatch:

# Concurrency control: Only run latest E2E tests for each PR/branch
concurrency:
  group: e2e-pr-${{ github.event.pull_request.number || github.run_id }}
  cancel-in-progress: true

permissions:
  contents: read
  pull-requests: write
  statuses: read
  checks: read

env:
  NODE_VERSION: 22.x
  DO_NOT_TRACK: 1

jobs:
  wait-for-test:
    name: Wait for Test workflow to complete
    runs-on: ubuntu-latest
    timeout-minutes: 45
    outputs:
      tests-passed: ${{ steps.check-tests.outputs.tests-passed }}
    steps:
      - name: Wait for Test workflow to succeed
        id: check-tests
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          echo "‚è≥ Waiting for Test workflow to complete..."
          COMMIT_SHA="${{ github.event.pull_request.head.sha || github.sha }}"
          echo "Commit SHA: $COMMIT_SHA"
          
          # Phase 1: Wait for Test workflow to START (check for "Get-Test-Groups" job - max 1 minute)
          echo "üìç Phase 1: Waiting for Test workflow to start..."
          for i in {1..12}; do
            GET_TEST_GROUPS_CHECK=$(gh api "/repos/${{ github.repository }}/commits/$COMMIT_SHA/check-runs" \
              --jq '.check_runs[] | select(.name == "Get-Test-Groups") | .id' 2>/dev/null || echo "")
            
            if [ -n "$GET_TEST_GROUPS_CHECK" ]; then
              echo "‚úÖ Test workflow started (Get-Test-Groups job found)!"
              break
            fi
            
            if [ $i -eq 12 ]; then
              echo "‚ùå Test workflow not found after 1 minute"
              echo "tests-passed=false" >> $GITHUB_OUTPUT
              exit 0
            fi
            
            echo "Attempt $i/12: Test workflow not started yet, waiting 5s..."
            sleep 5
          done
          
          # Phase 2: Wait for Test workflow to COMPLETE (check for "Tests" summary job - max 45 min total)
          echo "üìç Phase 2: Waiting for Test workflow to complete (Tests summary job)..."
          
          # First wait for "Tests" job to appear (it only appears after all other jobs finish)
          echo "Waiting for Tests summary job to appear..."
          while true; do
            TESTS_CHECK=$(gh api "/repos/${{ github.repository }}/commits/$COMMIT_SHA/check-runs" \
              --jq '.check_runs[] | select(.name == "Tests") | .id' 2>/dev/null || echo "")
            
            if [ -n "$TESTS_CHECK" ]; then
              echo "‚úÖ Tests summary job found!"
              break
            fi
            
            echo "Tests job not found yet (other tests still running), checking again in 30s..."
            sleep 30
          done
          
          # Now wait for Tests job to complete successfully
          echo "Waiting for Tests summary job to complete..."
          while true; do
            CHECK_STATUS=$(gh api "/repos/${{ github.repository }}/commits/$COMMIT_SHA/check-runs" \
              --jq '.check_runs[] | select(.name == "Tests") | "\(.status)|\(.conclusion)"' | head -n 1)
            
            STATUS=$(echo "$CHECK_STATUS" | cut -d'|' -f1 | tr -d '[:space:]')
            CONCLUSION=$(echo "$CHECK_STATUS" | cut -d'|' -f2 | tr -d '[:space:]')
            
            echo "Tests job status: [$STATUS], conclusion: [$CONCLUSION]"
            
            # Check if status is completed (using string matching to handle any edge cases)
            if [[ "$STATUS" == "completed" ]]; then
              if [[ "$CONCLUSION" == "success" ]]; then
                echo "‚úÖ Test workflow completed successfully!"
                echo "tests-passed=true" >> $GITHUB_OUTPUT
                exit 0
              elif [[ "$CONCLUSION" == "failure" ]]; then
                echo "‚ùå Tests failed - E2E tests will not run"
                echo "tests-passed=false" >> $GITHUB_OUTPUT
                exit 0
              elif [[ "$CONCLUSION" == "skipped" ]]; then
                echo "‚è≠Ô∏è  Tests were skipped - a required test job likely failed"
                echo "E2E tests will not run because tests did not complete successfully."
                echo "tests-passed=false" >> $GITHUB_OUTPUT
                exit 0
              elif [[ "$CONCLUSION" == "cancelled" ]]; then
                echo "üö´ Tests were cancelled by user or timeout"
                echo "E2E tests will not run."
                echo "tests-passed=false" >> $GITHUB_OUTPUT
                exit 0
              else
                echo "‚ùå Tests completed with unexpected conclusion: [$CONCLUSION]"
                echo "E2E tests will not run."
                echo "tests-passed=false" >> $GITHUB_OUTPUT
                exit 0
              fi
            fi
            
            echo "Tests job still running, checking again in 30s..."
            sleep 30
          done

  get-test-tags:
    needs: [wait-for-test]
    # Only run if tests actually passed
    if: needs.wait-for-test.outputs.tests-passed == 'true'
    runs-on: self-hosted
    outputs:
      test-tags: ${{ steps.set-tags.outputs.test-tags }}
    steps:
      - name: Set default test tags
        id: set-tags
        # Test sets:
        # @ci-dashboard-set-1: Pipelines (Import and Run), Workbenches (create with storage), Project Creation, User Login
        # @ci-dashboard-set-2: Model Stop/Start, Cluster Settings, Connection Creation, Cluster Storage Creation
        # TODO: Use turbo to run tests according to PR changes.
        run: |
          echo "test-tags=[\"@ci-dashboard-set-1\",\"@ci-dashboard-set-2\"]" >> $GITHUB_OUTPUT

  e2e-tests:
    needs: [get-test-tags]
    # Will automatically skip if get-test-tags was skipped
    runs-on: self-hosted
    timeout-minutes: 30
    strategy:
      fail-fast: false
      matrix:
        test-tag: ${{ fromJson(needs.get-test-tags.outputs.test-tags) }}
    steps:
      - name: Calculate unique port for this workflow run
        run: |
          # Dynamic port allocation for parallel execution
          BASE_PORT=$((4000 + (${{ github.run_id }} % 1000) * 5))
          
          # Add matrix offset to separate concurrent jobs within same PR
          if [[ "${{ matrix.test-tag }}" == *"set-1"* ]]; then
            MATRIX_OFFSET=0
          elif [[ "${{ matrix.test-tag }}" == *"set-2"* ]]; then
            MATRIX_OFFSET=1
          else
            MATRIX_OFFSET=2
          fi
          
          WEBPACK_PORT=$((BASE_PORT + MATRIX_OFFSET))
          
          # Store port info with run_id for cleanup tracking
          PORT_INFO_DIR="/tmp/gha-ports"
          mkdir -p "$PORT_INFO_DIR"
          echo "${{ github.run_id }}" > "$PORT_INFO_DIR/port-${WEBPACK_PORT}.run_id"
          
          echo "WEBPACK_PORT=$WEBPACK_PORT" >> $GITHUB_ENV
          echo "PORT_INFO_FILE=$PORT_INFO_DIR/port-${WEBPACK_PORT}.run_id" >> $GITHUB_ENV
          echo "üìç Using port ${WEBPACK_PORT} for ${{ matrix.test-tag }} (run_id: ${{ github.run_id }})"

      - name: Checkout code
        uses: actions/checkout@v4
        with:
          # For pull_request_target, explicitly checkout the PR head (from fork)
          # This is safe because we only run after test.yml passes
          ref: ${{ github.event.pull_request.head.sha || github.sha }}
          # Fetch from the fork repository if it's a fork PR
          repository: ${{ github.event.pull_request.head.repo.full_name || github.repository }}

      - name: Restore npm dependencies cache
        uses: actions/cache/restore@v4
        id: npm-cache
        with:
          path: |
            ~/.cache/Cypress
            **/node_modules
          key: ${{ runner.os }}-${{ env.NODE_VERSION }}-all-modules-${{ hashFiles('**/package-lock.json') }}
          restore-keys: |
            ${{ runner.os }}-${{ env.NODE_VERSION }}-all-modules-

      - name: Setup Node.js ${{ env.NODE_VERSION }}
        if: steps.npm-cache.outputs.cache-hit != 'true'
        uses: actions/setup-node@v4.3.0
        with:
          node-version: ${{ env.NODE_VERSION }}

      - name: Install dependencies
        if: steps.npm-cache.outputs.cache-hit != 'true'
        run: npm ci

      - name: Restore turbo build artifacts cache
        uses: actions/cache/restore@v4
        with:
          path: ${{ github.workspace }}/.turbo
          key: ${{ runner.os }}-${{ env.NODE_VERSION }}-turbo-${{ github.sha }}-e2e
          restore-keys: |
            ${{ runner.os }}-${{ env.NODE_VERSION }}-turbo-

      - name: Restore OpenShift CLI tarball cache
        uses: actions/cache/restore@v4
        id: oc-cache
        with:
          path: ${{ runner.temp }}/oc.tar.gz
          key: ${{ runner.os }}-oc-tarball-${{ env.OC_VERSION || '4.15.0' }}

      - name: Download test configuration
        run: |
          echo "üîß Downloading test configuration from GitLab..."
          
          if [ -n "${{ secrets.GITLAB_TOKEN }}" ] && [ -n "${{ secrets.GITLAB_TEST_VARS_URL }}" ]; then
            if curl -f -k -H "Authorization: Bearer ${{ secrets.GITLAB_TOKEN }}" \
                        "${{ secrets.GITLAB_TEST_VARS_URL }}" \
                        -o ${{ github.workspace }}/frontend/src/__tests__/cypress/test-variables.yml; then
              echo "‚úÖ Successfully downloaded test configuration"
            else
              echo "‚ùå Failed to download test configuration from GitLab"
              exit 1
            fi
          else
            echo "‚ö†Ô∏è  GitLab secrets not available (forked PR or secrets not configured)"
            echo "üí° Attempting to use fallback test-variables.yml if available"
            # Check if test-variables.yml already exists (e.g., from a previous step or cached)
            if [ ! -f "${{ github.workspace }}/frontend/src/__tests__/cypress/test-variables.yml" ]; then
              echo "‚ùå No test-variables.yml found and secrets not available"
              echo "üí° This workflow requires GITLAB_TOKEN and GITLAB_TEST_VARS_URL secrets"
              exit 1
            else
              echo "‚úÖ Using existing test-variables.yml file"
            fi
          fi
          
          # Update ODH_DASHBOARD_URL if secret is not available (for fork PRs)
          if [ -z "${{ secrets.ODH_DASHBOARD_URL }}" ]; then
            echo "‚ö†Ô∏è  ODH_DASHBOARD_URL secret not available (forked PR) - using localhost"
            sed -i.bak "s|ODH_DASHBOARD_URL:.*|ODH_DASHBOARD_URL: http://localhost:${WEBPACK_PORT}|" ${{ github.workspace }}/frontend/src/__tests__/cypress/test-variables.yml || true
          fi

      - name: Login to OpenShift cluster
        run: |
          # For forked PRs, skip OpenShift login since secrets are not available
          if [ -z "${{ secrets.OC_SERVER }}" ]; then
            echo "‚ö†Ô∏è  OpenShift secrets not available (forked PR) - skipping cluster login"
            exit 0
          fi

          # Read credentials from downloaded test-variables.yml
          TEST_VARS_FILE="${{ github.workspace }}/frontend/src/__tests__/cypress/test-variables.yml"

          # Extract OC_USERNAME from OCP_ADMIN_USER section
          OC_USERNAME=$(grep -A 10 "^OCP_ADMIN_USER:" "$TEST_VARS_FILE" | grep "USERNAME:" | head -1 | sed 's/.*USERNAME: //' | tr -d ' ')

          # Extract OC_PASSWORD from OCP_ADMIN_USER section
          OC_PASSWORD=$(grep -A 10 "^OCP_ADMIN_USER:" "$TEST_VARS_FILE" | grep "PASSWORD:" | head -1 | sed 's/.*PASSWORD: //' | tr -d ' ')

          # Mask sensitive data in logs
          echo "::add-mask::$OC_PASSWORD"
          echo "::add-mask::$OC_USERNAME"
          
          echo "Logging in to OpenShift cluster..."
          oc login -u "$OC_USERNAME" -p "$OC_PASSWORD" --server="${{ secrets.OC_SERVER }}" --insecure-skip-tls-verify > /dev/null 2>&1
          
          if [ $? -eq 0 ]; then
            echo "‚úÖ Successfully logged in to OpenShift cluster (dash-e2e-odh)"
          else
            echo "‚ùå Failed to login to OpenShift cluster"
            exit 1
          fi

          # Export OpenShift configuration for Cypress tests
          export KUBECONFIG="$HOME/.kube/config"
          echo "KUBECONFIG=$KUBECONFIG" >> $GITHUB_ENV

      - name: Override namespace values from secrets
        run: |
          TEST_VARS_FILE="${{ github.workspace }}/frontend/src/__tests__/cypress/test-variables.yml"
          
          echo "Overriding RHOAI namespaces with ODH namespaces from secrets..."
          
          # Helper function to set or update a key-value pair in YAML
          set_yaml_value() {
            local key="$1"
            local value="$2"
            if [ -z "$value" ]; then
              return
            fi
            # Remove leading/trailing whitespace from value
            value=$(echo "$value" | xargs)
            if grep -q "^${key}:" "$TEST_VARS_FILE"; then
              sed -i "s|^${key}:.*|${key}: ${value}|" "$TEST_VARS_FILE"
            else
              echo "${key}: ${value}" >> "$TEST_VARS_FILE"
            fi
          }
          
          # Read ODH namespace values from GitHub secret
          # This overrides the default RHOAI namespaces in test-variables.yml
          ODH_NAMESPACES="${{ secrets.ODH_NAMESPACES }}"
          
          if [ -z "$ODH_NAMESPACES" ]; then
            echo "‚ö†Ô∏è ODH_NAMESPACES secret not set, skipping namespace override"
            exit 0
          fi
          
          # Mask namespace values in logs
          echo "::add-mask::$ODH_NAMESPACES"
          
          echo "üìù Overriding namespaces with ODH values..."
          
          # Parse comma-separated values from secret
          # Format: OPERATOR_NAMESPACE,APPLICATIONS_NAMESPACE,NOTEBOOKS_NAMESPACE,OPERATOR_NAME,ODH_DASHBOARD_PROJECT_NAME
          IFS=',' read -r OPERATOR_NS APPLICATIONS_NS NOTEBOOKS_NS OPERATOR_NAME PROJECT_NAME <<< "$ODH_NAMESPACES"
          
          # Override RHOAI namespaces with ODH values
          set_yaml_value "PRODUCT" "ODH"
          set_yaml_value "OPERATOR_NAMESPACE" "$OPERATOR_NS"
          set_yaml_value "APPLICATIONS_NAMESPACE" "$APPLICATIONS_NS"
          set_yaml_value "MONITORING_NAMESPACE" "$APPLICATIONS_NS"
          set_yaml_value "NOTEBOOKS_NAMESPACE" "$NOTEBOOKS_NS"
          set_yaml_value "OPERATOR_NAME" "$OPERATOR_NAME"
          set_yaml_value "ODH_DASHBOARD_PROJECT_NAME" "$PROJECT_NAME"
          
          echo "‚úÖ Overridden namespaces with ODH values"
          echo "Namespace configuration updated"

      - name: Set test configuration
        run: |
          export CY_TEST_CONFIG="${{ github.workspace }}/frontend/src/__tests__/cypress/test-variables.yml"
          echo "CY_TEST_CONFIG=$CY_TEST_CONFIG" >> $GITHUB_ENV
          echo "‚úÖ Test configuration set (Cypress will connect to localhost:${WEBPACK_PORT})"

      - name: Start Cypress Server
        run: |
          echo "üßπ Cleaning up port ${WEBPACK_PORT}..."
          
          PORT_INFO_DIR="/tmp/gha-ports"
          PORT_INFO_FILE="$PORT_INFO_DIR/port-${WEBPACK_PORT}.run_id"
          CURRENT_RUN_ID="${{ github.run_id }}"
          
          # Check if port is in use
          if lsof -i:${WEBPACK_PORT} > /dev/null 2>&1; then
            # Check if there's a run_id file for this port
            if [ -f "$PORT_INFO_FILE" ]; then
              PORT_OWNER_RUN_ID=$(cat "$PORT_INFO_FILE")
              if [ "$PORT_OWNER_RUN_ID" != "$CURRENT_RUN_ID" ]; then
                echo "‚ö†Ô∏è  Port ${WEBPACK_PORT} is owned by different run_id: $PORT_OWNER_RUN_ID"
                echo "‚ö†Ô∏è  This port is in use by another workflow run - will not kill it"
                # Try to find an alternative port
                for alt_port in $(seq $((WEBPACK_PORT + 5)) $((WEBPACK_PORT + 50)) 5); do
                  if ! lsof -i:${alt_port} > /dev/null 2>&1; then
                    WEBPACK_PORT=$alt_port
                    PORT_INFO_FILE="$PORT_INFO_DIR/port-${WEBPACK_PORT}.run_id"
                    echo "‚úÖ Found alternative port: ${WEBPACK_PORT}"
                    break
                  fi
                done
              else
                echo "‚úÖ Port ${WEBPACK_PORT} is owned by this run - safe to clean up"
              fi
            else
              # No run_id file - check if process is from a recent GitHub Actions run
              PORT_PID=$(lsof -ti:${WEBPACK_PORT} 2>/dev/null | head -1)
              if [ -n "$PORT_PID" ]; then
                # Check if process is from a GitHub Actions workflow
                if ps -p "$PORT_PID" -o command= 2>/dev/null | grep -q "webpack.*serve\|node.*40[0-9][0-9]"; then
                  echo "‚ö†Ô∏è  Port ${WEBPACK_PORT} in use by potential GHA process (PID: $PORT_PID)"
                  echo "‚ö†Ô∏è  Being cautious - will not kill without run_id confirmation"
                  # Find alternative port
                  for alt_port in $(seq $((WEBPACK_PORT + 5)) $((WEBPACK_PORT + 50)) 5); do
                    if ! lsof -i:${alt_port} > /dev/null 2>&1; then
                      WEBPACK_PORT=$alt_port
                      PORT_INFO_FILE="$PORT_INFO_DIR/port-${WEBPACK_PORT}.run_id"
                      echo "‚úÖ Found alternative port: ${WEBPACK_PORT}"
                      break
                    fi
                  done
                else
                  echo "‚ö†Ô∏è  Port ${WEBPACK_PORT} in use by non-GHA process - cleaning up"
                  kill -9 "$PORT_PID" 2>/dev/null || true
                fi
              fi
            fi
          fi
          
          # Verify port is free with retry logic
          RETRY_COUNT=0
          while lsof -i:${WEBPACK_PORT} > /dev/null 2>&1; do
            RETRY_COUNT=$((RETRY_COUNT + 1))
            if [ $RETRY_COUNT -gt 10 ]; then
              echo "‚ùå Port ${WEBPACK_PORT} still in use after cleanup!"
              lsof -i:${WEBPACK_PORT}
              exit 1
            fi
            echo "‚è≥ Retrying cleanup... (attempt $RETRY_COUNT/10)"
            sleep 2
          done
          
          # Claim the port with our run_id
          mkdir -p "$PORT_INFO_DIR"
          echo "$CURRENT_RUN_ID" > "$PORT_INFO_FILE"
          echo "WEBPACK_PORT=$WEBPACK_PORT" >> $GITHUB_ENV
          echo "PORT_INFO_FILE=$PORT_INFO_FILE" >> $GITHUB_ENV
          
          echo "‚úÖ Port ${WEBPACK_PORT} is free and claimed by run_id: $CURRENT_RUN_ID"
          echo "üöÄ Starting webpack dev server on port ${WEBPACK_PORT} (dash-e2e-odh)..."
          
          # Start webpack and filter sensitive output
          cd frontend && ODH_PORT=${WEBPACK_PORT} npm run start:dev:ext > /tmp/webpack_${WEBPACK_PORT}.log 2>&1 &
          SERVER_PID=$!
          echo "SERVER_PID=$SERVER_PID" >> $GITHUB_ENV
          echo "$SERVER_PID" > "$PORT_INFO_DIR/port-${WEBPACK_PORT}.pid"
          
          # Give server time to initialize
          sleep 20
          
          # Show filtered webpack status (hide sensitive cluster URLs)
          if [ -f /tmp/webpack_${WEBPACK_PORT}.log ]; then
            tail -20 /tmp/webpack_${WEBPACK_PORT}.log | \
              grep -v "Dashboard host:" | \
              grep -v "Proxy created:" | \
              grep -v "Logged in as user:" | \
              grep -v "Using project:" || true
          fi

      - name: Wait for Server Ready
        run: |
          echo "‚è≥ Waiting for localhost:${WEBPACK_PORT} to be ready..."
          npx wait-on http://localhost:${WEBPACK_PORT} --timeout 120000
          
          # Verify the application loads with dashboard content
          for i in {1..10}; do
            if curl -s -f http://localhost:${WEBPACK_PORT}/ | grep -q "Data Science Projects\|ODH\|Open Data Hub\|Dashboard"; then
              echo "‚úÖ Server is ready and application is loaded!"
              break
            fi
            
            if [ $i -lt 10 ]; then
              echo "‚è≥ Waiting for application to load... (attempt $i/10)"
              sleep 8
            else
              echo "‚ùå Application failed to load properly after 10 attempts"
              exit 1
            fi
          done

      - name: Run E2E Tests
        run: |
          cd frontend

          echo "üß™ Running E2E tests for ${{ matrix.test-tag }}..."
          echo "üöÄ Running tests against live dashboard on port ${WEBPACK_PORT}"

          export CY_RESULTS_DIR="${{ github.workspace }}/frontend/src/__tests__/cypress/results/${{ matrix.test-tag }}"
          mkdir -p "$CY_RESULTS_DIR"

          # Run Cypress tests with Chrome browser
          # Videos are recorded and automatically uploaded on failure (default Cypress behavior)
          BASE_URL=http://localhost:${WEBPACK_PORT} npm run cypress:run:chrome -- \
            --env skipTags="@Bug @Maintain @NonConcurrent",grepTags="${{ matrix.test-tag }}",grepFilterSpecs=true \
            --config video=true,screenshotsFolder="$CY_RESULTS_DIR/screenshots",videosFolder="$CY_RESULTS_DIR/videos"

      - name: Test tag name
        if: ${{ always() }}
        run: |
          TEST_TAG_NAME=$(echo '${{ matrix.test-tag }}' | tr '/' '_' | tr '@' '_')
          echo "TEST_TAG_NAME=$TEST_TAG_NAME" >> $GITHUB_ENV

      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: e2e-test-results-${{ matrix.test-tag }}
          path: |
            frontend/src/__tests__/cypress/results/
            frontend/src/__tests__/cypress/videos/
            frontend/src/__tests__/cypress/screenshots/
            frontend/src/__tests__/cypress/coverage/
          retention-days: 7

      - name: Log test completion
        if: always()
        run: |
          echo "üèÅ E2E Test completed!"
          echo "Status: ${{ job.status }}"
          echo "Event: ${{ github.event_name }}"
          echo "Branch: ${{ github.ref_name }}"
          echo "Test Tag: ${{ matrix.test-tag }}"
          echo "Commit: ${{ github.sha }}"
          echo "Run ID: ${{ github.run_id }}"
          echo ""
          echo "üìä Test artifacts uploaded to:"
          echo "${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"

  cleanup-server:
    needs: [e2e-tests]
    runs-on: self-hosted
    if: ${{ always() && needs.e2e-tests.result != 'skipped' }}
    steps:
      - name: Stop Cypress Servers
        run: |
          echo "üõë Stopping webpack dev server for run_id: ${{ github.run_id }}..."
          
          PORT_INFO_DIR="/tmp/gha-ports"
          CURRENT_RUN_ID="${{ github.run_id }}"
          KILLED_COUNT=0
          
          # Find all port files owned by this run_id
          if [ -d "$PORT_INFO_DIR" ]; then
            for port_file in "$PORT_INFO_DIR"/port-*.run_id; do
              if [ -f "$port_file" ]; then
                PORT_OWNER_RUN_ID=$(cat "$port_file")
                if [ "$PORT_OWNER_RUN_ID" = "$CURRENT_RUN_ID" ]; then
                  # Extract port number from filename
                  PORT=$(basename "$port_file" | sed 's/port-\([0-9]*\)\.run_id/\1/')
                  PID_FILE="$PORT_INFO_DIR/port-${PORT}.pid"
                  
                  # Kill process if PID file exists
                  if [ -f "$PID_FILE" ]; then
                    PID=$(cat "$PID_FILE")
                    if ps -p "$PID" > /dev/null 2>&1; then
                      echo "üõë Killing process $PID on port $PORT (run_id: $CURRENT_RUN_ID)"
                      # Kill the main process and all its children (including Chrome processes spawned by Cypress)
                      pkill -P "$PID" 2>/dev/null || true
                      kill "$PID" 2>/dev/null || true
                      KILLED_COUNT=$((KILLED_COUNT + 1))
                    fi
                  fi
                  
                  # Also kill any process on this port (double-check)
                  PORT_PID=$(lsof -ti:${PORT} 2>/dev/null | head -1)
                  if [ -n "$PORT_PID" ]; then
                    echo "üõë Killing process $PORT_PID on port $PORT"
                    # Kill children processes too
                    pkill -P "$PORT_PID" 2>/dev/null || true
                    kill "$PORT_PID" 2>/dev/null || true
                  fi
                  
                  # Clean up any orphaned Chrome processes that might be related to Cypress
                  ALL_PORT_PIDS=$(lsof -ti:${PORT} 2>/dev/null || true)
                  if [ -n "$ALL_PORT_PIDS" ]; then
                    for port_pid in $ALL_PORT_PIDS; do
                      # Check if this is a Chrome/Chromium process
                      if ps -p "$port_pid" -o comm= 2>/dev/null | grep -qE "chrome|chromium"; then
                        echo "üõë Killing Chrome process $port_pid (using port $PORT)"
                        # Kill the Chrome process and all its children
                        pkill -P "$port_pid" 2>/dev/null || true
                        kill "$port_pid" 2>/dev/null || true
                      fi
                    done
                  fi
                  
                  # Also look for Chrome processes that are children of our webpack process
                  if [ -n "$PID" ] && ps -p "$PID" > /dev/null 2>&1; then
                    CHROME_CHILDREN=$(pgrep -P "$PID" -f "chrome|chromium" 2>/dev/null || true)
                    if [ -n "$CHROME_CHILDREN" ]; then
                      echo "üßπ Cleaning up Chrome processes spawned by webpack (PID $PID)..."
                      for chrome_pid in $CHROME_CHILDREN; do
                        echo "üõë Killing Chrome process $chrome_pid (child of webpack PID $PID)"
                        pkill -P "$chrome_pid" 2>/dev/null || true
                        kill "$chrome_pid" 2>/dev/null || true
                      done
                    fi
                  fi
                  
                  # Fallback: Look for Chrome processes with Cypress-specific flags that might be orphaned
                  # These are Chrome processes that Cypress spawned but might not be directly related to our port
                  # We only kill them if they're accessing our port to be safe
                  ORPHANED_CHROME=$(pgrep -f "chrome.*--test-type|chromium.*--test-type" 2>/dev/null || true)
                  if [ -n "$ORPHANED_CHROME" ]; then
                    for chrome_pid in $ORPHANED_CHROME; do
                      # Only kill if it's accessing our port (to avoid killing other users' Chrome processes)
                      if lsof -p "$chrome_pid" 2>/dev/null | grep -q ":${PORT}"; then
                        echo "üõë Killing orphaned Chrome process $chrome_pid (accessing port $PORT)"
                        pkill -P "$chrome_pid" 2>/dev/null || true
                        kill "$chrome_pid" 2>/dev/null || true
                      fi
                    done
                  fi
                  
                  # Clean up port info files
                  rm -f "$port_file" "$PID_FILE"
                fi
              fi
            done
          fi
          
          # Clean up any stale port files older than 24 hours
          find "$PORT_INFO_DIR" -name "*.run_id" -mtime +1 -delete 2>/dev/null || true
          find "$PORT_INFO_DIR" -name "*.pid" -mtime +1 -delete 2>/dev/null || true
          
          if [ $KILLED_COUNT -eq 0 ]; then
            echo "‚úÖ No processes found for run_id: $CURRENT_RUN_ID"
          else
            echo "‚úÖ Cleaned up $KILLED_COUNT process(es) for run_id: $CURRENT_RUN_ID"
          fi


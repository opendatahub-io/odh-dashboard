name: Cypress e2e Test

# =============================================================================
# E2E Test Workflow with Cluster Failover and Dynamic Tag Selection
# =============================================================================
#
# TRIGGERS:
#   - Automatically after "Test" workflow completes on PRs
#   - Manually via workflow_dispatch (Actions tab ‚Üí Run workflow)
#
# CLUSTER FAILOVER:
#   Primary:   dash-e2e-int (checked first via DSC health)
#   Secondary: dash-e2e     (used if primary is unhealthy)
#   Health Check: Logs into cluster ‚Üí checks DSC conditions (Available, Degraded, odh-dashboardReady)
#
# TEST SELECTION:
#   Default (always run):
#     - @ci-dashboard-set-1
#     - @ci-dashboard-set-2
#
#   Run additional tests via PR labels:
#     Add labels with 'test:' prefix to your PR:
#       test:Pipelines    ‚Üí @Pipelines
#       test:ModelServing ‚Üí @ModelServing
#       test:Workbenches  ‚Üí @Workbenches
#     Any 'test:<TagName>' label maps to '@<TagName>' Cypress grep tag
#
#   Run additional tests manually:
#     1. Go to Actions tab ‚Üí "Cypress e2e Test" workflow
#     2. Click "Run workflow"
#     3. Enter tags in 'additional_tags' field: @Pipelines,@Workbenches
#
# LIMITS:
#   - Max 5 additional tags beyond defaults (prevents runner exhaustion)
#   - 10 runners shared across 30+ devs
#
# REQUIRED SECRETS:
#   PRIMARY:   OC_SERVER_PRIMARY, OCP_CONSOLE_URL_PRIMARY, ODH_DASHBOARD_URL_PRIMARY
#   SECONDARY: OC_SERVER, OCP_CONSOLE_URL, ODH_DASHBOARD_URL
#   AUTH:      GITLAB_TOKEN, GITLAB_TEST_VARS_URL, ODH_NAMESPACES
# =============================================================================

on:
  workflow_run:
    workflows: ["Test"]
    types: [completed]

  workflow_dispatch:
    inputs:
      additional_tags:
        description: 'Extra test tags (e.g., @Pipelines,@Workbenches)'
        required: false
        default: ''
        type: string

concurrency:
  group: e2e-${{ github.event.workflow_run.head_branch || github.ref }}
  cancel-in-progress: true

permissions:
  contents: read
  pull-requests: read
  actions: read
  statuses: write

env:
  NODE_VERSION: 22.x
  DO_NOT_TRACK: 1

# =============================================================================
# JOBS
# =============================================================================
jobs:

  # ---------------------------------------------------------------------------
  # Cluster Selection - Health check with automatic failover
  # ---------------------------------------------------------------------------
  select-cluster:
    if: >-
      github.event_name == 'workflow_dispatch' || 
      (github.event.workflow_run.event == 'pull_request' && 
       github.event.workflow_run.conclusion == 'success')
    runs-on: self-hosted
    outputs:
      cluster_url: ${{ steps.select.outputs.cluster_url }}
      cluster_name: ${{ steps.select.outputs.cluster_name }}
      dashboard_url: ${{ steps.select.outputs.dashboard_url }}
    steps:
      - name: Download test credentials
        run: |
          echo "üîß Downloading test credentials for cluster health check..."
          curl -fk -H "Authorization: Bearer ${{ secrets.GITLAB_TOKEN }}" \
            "${{ secrets.GITLAB_TEST_VARS_URL }}" \
            -o /tmp/test-variables.yml
          echo "‚úÖ Downloaded test credentials"

      - name: Select healthy cluster
        id: select
        env:
          PRIMARY_SERVER: ${{ secrets.OC_SERVER_PRIMARY }}
          PRIMARY_DASHBOARD: ${{ secrets.ODH_DASHBOARD_URL_PRIMARY }}
          SECONDARY_SERVER: ${{ secrets.OC_SERVER }}
          SECONDARY_DASHBOARD: ${{ secrets.ODH_DASHBOARD_URL }}
        run: |
          # Extract credentials from test-variables.yml
          TEST_VARS_FILE="/tmp/test-variables.yml"
          OC_USERNAME=$(grep -A 10 "^OCP_ADMIN_USER:" "$TEST_VARS_FILE" | grep "USERNAME:" | head -1 | sed 's/.*USERNAME: //' | tr -d ' ')
          OC_PASSWORD=$(grep -A 10 "^OCP_ADMIN_USER:" "$TEST_VARS_FILE" | grep "PASSWORD:" | head -1 | sed 's/.*PASSWORD: //' | tr -d ' ')
          echo "::add-mask::$OC_PASSWORD"
          echo "::add-mask::$OC_USERNAME"

          # Check DSC health by logging in and verifying conditions
          check_dsc_health() {
            local server_url="$1"
            local cluster_name="$2"
            
            [[ -z "$server_url" ]] && return 1
            
            # Try to login
            if ! oc login -u "$OC_USERNAME" -p "$OC_PASSWORD" --server="$server_url" --insecure-skip-tls-verify > /dev/null 2>&1; then
              echo "  ‚ùå Failed to login to $cluster_name"
              return 1
            fi
            
            # Get DSC status
            DSC_JSON=$(oc get datasciencecluster -o json 2>/dev/null)
            if [[ -z "$DSC_JSON" || "$DSC_JSON" == "null" ]]; then
              echo "  ‚ùå No DataScienceCluster found on $cluster_name"
              return 1
            fi
            
            # Check phase (quickest check)
            PHASE=$(echo "$DSC_JSON" | jq -r '.items[0].status.phase // "Unknown"')
            if [[ "$PHASE" == "Ready" ]]; then
              echo "  ‚úÖ DSC phase: Ready"
            else
              echo "  ‚ö†Ô∏è DSC phase: $PHASE (expected Ready)"
              # Continue checking conditions for more detail
            fi
            
            # Check critical conditions: Available=True, Degraded!=True, odh-dashboardReady=True
            AVAILABLE=$(echo "$DSC_JSON" | jq -r '.items[0].status.conditions[] | select(.type=="Available") | .status' 2>/dev/null)
            DEGRADED=$(echo "$DSC_JSON" | jq -r '.items[0].status.conditions[] | select(.type=="Degraded") | .status' 2>/dev/null)
            DASHBOARD_READY=$(echo "$DSC_JSON" | jq -r '.items[0].status.conditions[] | select(.type=="odh-dashboardReady") | .status' 2>/dev/null)
            
            echo "  üìä Conditions: Available=$AVAILABLE, Degraded=$DEGRADED, DashboardReady=$DASHBOARD_READY"
            
            # Cluster is healthy if: Available=True AND Degraded!=True AND (DashboardReady=True OR not set)
            if [[ "$AVAILABLE" == "True" && "$DEGRADED" != "True" ]]; then
              if [[ "$DASHBOARD_READY" == "True" || -z "$DASHBOARD_READY" ]]; then
                return 0
              else
                echo "  ‚ö†Ô∏è Dashboard not ready yet"
                return 1
              fi
            fi
            
            echo "  ‚ùå DSC conditions not healthy"
            return 1
          }

          echo "üîç Checking PRIMARY cluster (dash-e2e-int)..."
          if check_dsc_health "$PRIMARY_SERVER" "dash-e2e-int"; then
            echo "‚úÖ PRIMARY cluster is healthy and ready"
            echo "cluster_url=$PRIMARY_SERVER" >> $GITHUB_OUTPUT
            echo "cluster_name=dash-e2e-int" >> $GITHUB_OUTPUT
            echo "dashboard_url=$PRIMARY_DASHBOARD" >> $GITHUB_OUTPUT
          else
            echo ""
            echo "‚ö†Ô∏è PRIMARY unavailable or not ready, trying SECONDARY (dash-e2e)..."
            if check_dsc_health "$SECONDARY_SERVER" "dash-e2e"; then
              echo "‚úÖ SECONDARY cluster is healthy and ready"
              echo "cluster_url=$SECONDARY_SERVER" >> $GITHUB_OUTPUT
              echo "cluster_name=dash-e2e" >> $GITHUB_OUTPUT
              echo "dashboard_url=$SECONDARY_DASHBOARD" >> $GITHUB_OUTPUT
            else
              echo ""
              echo "‚ùå All clusters unavailable or unhealthy"
              exit 1
            fi
          fi
          
          # Clean up credentials file
          rm -f /tmp/test-variables.yml

  # ---------------------------------------------------------------------------
  # Status - Set pending status on PR
  # ---------------------------------------------------------------------------
  set-pending-status:
    needs: [select-cluster]
    runs-on: ubuntu-latest
    steps:
      - name: Set pending status
        env:
          GH_TOKEN: ${{ github.token }}
        run: |
          gh api repos/${{ github.repository }}/statuses/${{ github.event.workflow_run.head_sha || github.sha }} \
            -f state=pending \
            -f context="Cypress E2E Tests" \
            -f description="Running on ${{ needs.select-cluster.outputs.cluster_name }}" \
            -f target_url="${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"

  # ---------------------------------------------------------------------------
  # Tag Resolution - Build test matrix from defaults + PR labels/input
  # ---------------------------------------------------------------------------
  get-test-tags:
    needs: [select-cluster]
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.build.outputs.matrix }}
      source: ${{ steps.build.outputs.source }}
    steps:
      - name: Get PR labels
        id: labels
        if: github.event_name == 'workflow_run'
        env:
          GH_TOKEN: ${{ github.token }}
        run: |
          # Get PR number
          PR_NUM="${{ github.event.workflow_run.pull_requests[0].number }}"
          if [[ -z "$PR_NUM" || "$PR_NUM" == "null" ]]; then
            PR_NUM=$(gh api "repos/${{ github.repository }}/commits/${{ github.event.workflow_run.head_sha }}/pulls" \
              --jq '.[0].number' 2>/dev/null || echo "")
          fi

          if [[ -n "$PR_NUM" && "$PR_NUM" != "null" ]]; then
            LABELS=$(gh api "repos/${{ github.repository }}/issues/$PR_NUM/labels" \
              --jq '[.[].name] | join(",")' 2>/dev/null || echo "")
            echo "labels=$LABELS" >> $GITHUB_OUTPUT
            echo "üìã PR #$PR_NUM labels: $LABELS"
          fi

      - name: Build test matrix
        id: build
        run: |
          # Configuration
          MAX_EXTRA_TAGS=5  # Limit additional tags to prevent runner exhaustion
          
          # Defaults - these ALWAYS run
          TAGS="@ci-dashboard-set-1,@ci-dashboard-set-2"
          SOURCE="default"
          EXTRA_COUNT=0

          # Priority 1: Manual input (workflow_dispatch)
          if [[ -n "${{ inputs.additional_tags }}" ]]; then
            for tag in $(echo "${{ inputs.additional_tags }}" | tr ',' ' '); do
              if [[ $EXTRA_COUNT -lt $MAX_EXTRA_TAGS ]]; then
                TAGS="$TAGS,$tag"
                EXTRA_COUNT=$((EXTRA_COUNT + 1))
              fi
            done
            SOURCE="manual"
            echo "üìù Added manual tags (limit: $MAX_EXTRA_TAGS)"

          # Priority 2: PR labels (test:* pattern)
          elif [[ -n "${{ steps.labels.outputs.labels }}" ]]; then
            for label in $(echo "${{ steps.labels.outputs.labels }}" | tr ',' ' '); do
              if [[ "$label" == test:* && $EXTRA_COUNT -lt $MAX_EXTRA_TAGS ]]; then
                tag="@${label#test:}"
                tag="${tag#@}"  # Remove double @
                tag="@$tag"
                TAGS="$TAGS,$tag"
                EXTRA_COUNT=$((EXTRA_COUNT + 1))
                SOURCE="pr-labels"
                echo "üè∑Ô∏è Label '$label' ‚Üí $tag"
              fi
            done
          fi

          if [[ $EXTRA_COUNT -ge $MAX_EXTRA_TAGS ]]; then
            echo "‚ö†Ô∏è Tag limit reached ($MAX_EXTRA_TAGS max). Some tags were not added."
          fi

          # Convert to JSON matrix (deduplicated)
          MATRIX=$(echo "$TAGS" | tr ',' '\n' | sort -u | grep -v '^$' | \
            sed 's/^[^@]/@&/' | jq -Rc '[., inputs] | unique' | jq -sc 'add | unique')
          
          echo "matrix=$MATRIX" >> $GITHUB_OUTPUT
          echo "source=$SOURCE" >> $GITHUB_OUTPUT
          echo "üß™ Final matrix: $MATRIX (source: $SOURCE)"

  # ---------------------------------------------------------------------------
  # E2E Tests - Run Cypress tests for each tag in parallel
  # ---------------------------------------------------------------------------
  e2e-tests:
    needs: [select-cluster, set-pending-status, get-test-tags]
    runs-on: self-hosted
    timeout-minutes: 30
    strategy:
      fail-fast: false
      matrix:
        tag: ${{ fromJson(needs.get-test-tags.outputs.matrix) }}
    env:
      CLUSTER_NAME: ${{ needs.select-cluster.outputs.cluster_name }}
      CLUSTER_URL: ${{ needs.select-cluster.outputs.cluster_url }}
      DASHBOARD_URL: ${{ needs.select-cluster.outputs.dashboard_url }}
    steps:
      - name: Calculate unique port for this workflow run
        run: |
          # Dynamic port allocation for parallel execution
          BASE_PORT=$((4000 + (${{ github.run_id }} % 1000) * 5))

          # Add matrix offset to separate concurrent jobs within same PR
          if [[ "${{ matrix.tag }}" == *"set-1"* ]]; then
            MATRIX_OFFSET=0
          elif [[ "${{ matrix.tag }}" == *"set-2"* ]]; then
            MATRIX_OFFSET=1
          else
            MATRIX_OFFSET=2
          fi

          WEBPACK_PORT=$((BASE_PORT + MATRIX_OFFSET))

          # Store port info with run_id for cleanup tracking
          PORT_INFO_DIR="/tmp/gha-ports"
          mkdir -p "$PORT_INFO_DIR"
          echo "${{ github.run_id }}" > "$PORT_INFO_DIR/port-${WEBPACK_PORT}.run_id"

          echo "WEBPACK_PORT=$WEBPACK_PORT" >> $GITHUB_ENV
          echo "PORT_INFO_FILE=$PORT_INFO_DIR/port-${WEBPACK_PORT}.run_id" >> $GITHUB_ENV
          echo "üìç Using port ${WEBPACK_PORT} for ${{ matrix.tag }} (run_id: ${{ github.run_id }})"

      - name: Checkout code
        uses: actions/checkout@v4
        with:
          ref: ${{ github.event.workflow_run.head_sha || github.sha }}

      - name: Restore npm dependencies cache
        uses: actions/cache/restore@v4
        id: npm-cache
        with:
          path: |
            ~/.cache/Cypress
            **/node_modules
          key: ${{ runner.os }}-${{ env.NODE_VERSION }}-all-modules-${{ hashFiles('**/package-lock.json') }}
          restore-keys: |
            ${{ runner.os }}-${{ env.NODE_VERSION }}-all-modules-

      - name: Setup Node.js ${{ env.NODE_VERSION }}
        if: steps.npm-cache.outputs.cache-hit != 'true'
        uses: actions/setup-node@v4.3.0
        with:
          node-version: ${{ env.NODE_VERSION }}

      - name: Install dependencies
        if: steps.npm-cache.outputs.cache-hit != 'true'
        run: npm ci

      - name: Restore turbo build artifacts cache
        uses: actions/cache/restore@v4
        with:
          path: ${{ github.workspace }}/.turbo
          key: ${{ runner.os }}-${{ env.NODE_VERSION }}-turbo-${{ github.sha }}-e2e
          restore-keys: |
            ${{ runner.os }}-${{ env.NODE_VERSION }}-turbo-

      - name: Restore OpenShift CLI tarball cache
        uses: actions/cache/restore@v4
        id: oc-cache
        with:
          path: ${{ runner.temp }}/oc.tar.gz
          key: ${{ runner.os }}-oc-tarball-${{ env.OC_VERSION || '4.15.0' }}

      - name: Download test configuration
        run: |
          echo "üîß Downloading test configuration from GitLab..."
          curl -fk -H "Authorization: Bearer ${{ secrets.GITLAB_TOKEN }}" \
                        "${{ secrets.GITLAB_TEST_VARS_URL }}" \
            -o ${{ github.workspace }}/packages/cypress/test-variables.yml
          echo "‚úÖ Downloaded test configuration"

      - name: Login to OpenShift cluster
        run: |
          TEST_VARS_FILE="${{ github.workspace }}/packages/cypress/test-variables.yml"

          # Extract credentials
          OC_USERNAME=$(grep -A 10 "^OCP_ADMIN_USER:" "$TEST_VARS_FILE" | grep "USERNAME:" | head -1 | sed 's/.*USERNAME: //' | tr -d ' ')
          OC_PASSWORD=$(grep -A 10 "^OCP_ADMIN_USER:" "$TEST_VARS_FILE" | grep "PASSWORD:" | head -1 | sed 's/.*PASSWORD: //' | tr -d ' ')
          echo "::add-mask::$OC_PASSWORD"
          echo "::add-mask::$OC_USERNAME"

          echo "Logging in to OpenShift cluster ($CLUSTER_NAME)..."
          oc login -u "$OC_USERNAME" -p "$OC_PASSWORD" --server="$CLUSTER_URL" --insecure-skip-tls-verify > /dev/null 2>&1

          if [ $? -eq 0 ]; then
            echo "‚úÖ Successfully logged in to $CLUSTER_NAME"
          else
            echo "‚ùå Failed to login to OpenShift cluster"
            exit 1
          fi

          echo "KUBECONFIG=$HOME/.kube/config" >> $GITHUB_ENV

      - name: Override namespace values
        run: |
          TEST_VARS_FILE="${{ github.workspace }}/packages/cypress/test-variables.yml"
          ODH_NAMESPACES="${{ secrets.ODH_NAMESPACES }}"

          # Set dashboard URL for selected cluster
          if [ -n "$DASHBOARD_URL" ]; then
            sed -i "s|^ODH_DASHBOARD_URL:.*|ODH_DASHBOARD_URL: $DASHBOARD_URL|" "$TEST_VARS_FILE"
          fi

          if [ -z "$ODH_NAMESPACES" ]; then
            echo "‚ö†Ô∏è ODH_NAMESPACES secret not set, skipping namespace override"
            exit 0
          fi

          echo "::add-mask::$ODH_NAMESPACES"
          echo "üìù Overriding namespaces with ODH values..."

          IFS=',' read -r OPERATOR_NS APPLICATIONS_NS NOTEBOOKS_NS OPERATOR_NAME PROJECT_NAME <<< "$ODH_NAMESPACES"

          sed -i "s|^PRODUCT:.*|PRODUCT: ODH|" "$TEST_VARS_FILE"
          sed -i "s|^OPERATOR_NAMESPACE:.*|OPERATOR_NAMESPACE: $OPERATOR_NS|" "$TEST_VARS_FILE"
          sed -i "s|^APPLICATIONS_NAMESPACE:.*|APPLICATIONS_NAMESPACE: $APPLICATIONS_NS|" "$TEST_VARS_FILE"
          sed -i "s|^MONITORING_NAMESPACE:.*|MONITORING_NAMESPACE: $APPLICATIONS_NS|" "$TEST_VARS_FILE"
          sed -i "s|^NOTEBOOKS_NAMESPACE:.*|NOTEBOOKS_NAMESPACE: $NOTEBOOKS_NS|" "$TEST_VARS_FILE"
          sed -i "s|^OPERATOR_NAME:.*|OPERATOR_NAME: $OPERATOR_NAME|" "$TEST_VARS_FILE"
          sed -i "s|^ODH_DASHBOARD_PROJECT_NAME:.*|ODH_DASHBOARD_PROJECT_NAME: $PROJECT_NAME|" "$TEST_VARS_FILE"

          echo "‚úÖ Namespace configuration updated"

      - name: Set test configuration
        run: |
          echo "CY_TEST_CONFIG=${{ github.workspace }}/packages/cypress/test-variables.yml" >> $GITHUB_ENV

      - name: Start Cypress Server
        run: |
          echo "üßπ Cleaning up port ${WEBPACK_PORT}..."

          PORT_INFO_DIR="/tmp/gha-ports"
          PORT_INFO_FILE="$PORT_INFO_DIR/port-${WEBPACK_PORT}.run_id"
          CURRENT_RUN_ID="${{ github.run_id }}"

          # Check if port is in use
          if lsof -i:${WEBPACK_PORT} > /dev/null 2>&1; then
            # Check if there's a run_id file for this port
            if [ -f "$PORT_INFO_FILE" ]; then
              PORT_OWNER_RUN_ID=$(cat "$PORT_INFO_FILE")
              if [ "$PORT_OWNER_RUN_ID" != "$CURRENT_RUN_ID" ]; then
                echo "‚ö†Ô∏è  Port ${WEBPACK_PORT} is owned by different run_id: $PORT_OWNER_RUN_ID"
                echo "‚ö†Ô∏è  This port is in use by another workflow run - will not kill it"
                # Try to find an alternative port
                for alt_port in $(seq $((WEBPACK_PORT + 5)) $((WEBPACK_PORT + 50)) 5); do
                  if ! lsof -i:${alt_port} > /dev/null 2>&1; then
                    WEBPACK_PORT=$alt_port
                    PORT_INFO_FILE="$PORT_INFO_DIR/port-${WEBPACK_PORT}.run_id"
                    echo "‚úÖ Found alternative port: ${WEBPACK_PORT}"
                    break
                  fi
                done
              else
                echo "‚úÖ Port ${WEBPACK_PORT} is owned by this run - safe to clean up"
              fi
            else
              # No run_id file - check if process is from a recent GitHub Actions run
              PORT_PID=$(lsof -ti:${WEBPACK_PORT} 2>/dev/null | head -1)
              if [ -n "$PORT_PID" ]; then
                # Check if process is from a GitHub Actions workflow
                if ps -p "$PORT_PID" -o command= 2>/dev/null | grep -q "webpack.*serve\|node.*40[0-9][0-9]"; then
                  echo "‚ö†Ô∏è  Port ${WEBPACK_PORT} in use by potential GHA process (PID: $PORT_PID)"
                  echo "‚ö†Ô∏è  Being cautious - will not kill without run_id confirmation"
                  # Find alternative port
                  for alt_port in $(seq $((WEBPACK_PORT + 5)) $((WEBPACK_PORT + 50)) 5); do
                    if ! lsof -i:${alt_port} > /dev/null 2>&1; then
                      WEBPACK_PORT=$alt_port
                      PORT_INFO_FILE="$PORT_INFO_DIR/port-${WEBPACK_PORT}.run_id"
                      echo "‚úÖ Found alternative port: ${WEBPACK_PORT}"
                      break
                    fi
                  done
                else
                  echo "‚ö†Ô∏è  Port ${WEBPACK_PORT} in use by non-GHA process - cleaning up"
                  kill -9 "$PORT_PID" 2>/dev/null || true
                fi
              fi
            fi
          fi

          # Verify port is free with retry logic
          RETRY_COUNT=0
          while lsof -i:${WEBPACK_PORT} > /dev/null 2>&1; do
            RETRY_COUNT=$((RETRY_COUNT + 1))
            if [ $RETRY_COUNT -gt 10 ]; then
              echo "‚ùå Port ${WEBPACK_PORT} still in use after cleanup!"
              lsof -i:${WEBPACK_PORT}
              exit 1
            fi
            echo "‚è≥ Retrying cleanup... (attempt $RETRY_COUNT/10)"
            sleep 2
          done

          # Claim the port with our run_id
          mkdir -p "$PORT_INFO_DIR"
          echo "$CURRENT_RUN_ID" > "$PORT_INFO_FILE"
          echo "WEBPACK_PORT=$WEBPACK_PORT" >> $GITHUB_ENV
          echo "PORT_INFO_FILE=$PORT_INFO_FILE" >> $GITHUB_ENV

          echo "‚úÖ Port ${WEBPACK_PORT} is free and claimed by run_id: $CURRENT_RUN_ID"
          echo "üöÄ Starting webpack dev server on port ${WEBPACK_PORT} ($CLUSTER_NAME)..."

          # Start webpack and filter sensitive output
          cd frontend && ODH_PORT=${WEBPACK_PORT} npm run start:dev:ext > /tmp/webpack_${WEBPACK_PORT}.log 2>&1 &
          SERVER_PID=$!
          echo "SERVER_PID=$SERVER_PID" >> $GITHUB_ENV
          echo "$SERVER_PID" > "$PORT_INFO_DIR/port-${WEBPACK_PORT}.pid"

          # Give server time to initialize
          sleep 20

          # Show filtered webpack status (hide sensitive cluster URLs)
          if [ -f /tmp/webpack_${WEBPACK_PORT}.log ]; then
            tail -20 /tmp/webpack_${WEBPACK_PORT}.log | \
              grep -v "Dashboard host:" | \
              grep -v "Proxy created:" | \
              grep -v "Logged in as user:" | \
              grep -v "Using project:" || true
          fi

      - name: Wait for Server Ready
        run: |
          echo "‚è≥ Waiting for localhost:${WEBPACK_PORT} to be ready..."
          npx wait-on http://localhost:${WEBPACK_PORT} --timeout 120000

          # Verify the application loads with dashboard content
          for i in {1..10}; do
            if curl -s -f http://localhost:${WEBPACK_PORT}/ | grep -q "Data Science Projects\|ODH\|Open Data Hub\|Dashboard"; then
              echo "‚úÖ Server is ready and application is loaded!"
              break
            fi
            
            if [ $i -lt 10 ]; then
              echo "‚è≥ Waiting for application to load... (attempt $i/10)"
              sleep 8
            else
              echo "‚ùå Application failed to load properly after 10 attempts"
              exit 1
            fi
          done

      - name: Run E2E Tests
        run: |
          cd frontend

          echo "üß™ Running E2E tests for ${{ matrix.tag }}..."
          echo "üöÄ Running tests against live dashboard on port ${WEBPACK_PORT}"
          echo "üìå Tag source: ${{ needs.get-test-tags.outputs.source }}"

          export CY_RESULTS_DIR="${{ github.workspace }}/packages/cypress/results/${{ matrix.tag }}"
          mkdir -p "$CY_RESULTS_DIR"

          BASE_URL=http://localhost:${WEBPACK_PORT} npm run cypress:run:chrome -- \
            --env skipTags="@Bug @Maintain @NonConcurrent",grepTags="${{ matrix.tag }}",grepFilterSpecs=true \
            --config video=true,screenshotsFolder="$CY_RESULTS_DIR/screenshots",videosFolder="$CY_RESULTS_DIR/videos"

      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: e2e-results-${{ matrix.tag }}
          path: |
            packages/cypress/results/
            packages/cypress/videos/
            packages/cypress/screenshots/
          retention-days: 7

      - name: Log test completion
        if: always()
        run: |
          echo "üèÅ E2E Test completed!"
          echo "Status: ${{ job.status }}"
          echo "Test Tag: ${{ matrix.tag }}"
          echo "Cluster: $CLUSTER_NAME"
          echo "Run ID: ${{ github.run_id }}"

  # ---------------------------------------------------------------------------
  # Final Status - Update PR with test results
  # ---------------------------------------------------------------------------
  set-final-status:
    needs: [select-cluster, e2e-tests]
    if: always()
    runs-on: ubuntu-latest
    steps:
      - name: Set final status
        env:
          GH_TOKEN: ${{ github.token }}
        run: |
          RESULT="${{ needs.e2e-tests.result }}"
          CLUSTER="${{ needs.select-cluster.outputs.cluster_name }}"
          
          case "$RESULT" in
            success)   STATE="success"; DESC="All tests passed on $CLUSTER" ;;
            cancelled) STATE="error";   DESC="Tests cancelled" ;;
            skipped)   exit 0 ;;
            *)         STATE="failure"; DESC="Tests failed on $CLUSTER" ;;
          esac
          
          gh api repos/${{ github.repository }}/statuses/${{ github.event.workflow_run.head_sha || github.sha }} \
            -f state="$STATE" \
            -f context="Cypress E2E Tests" \
            -f description="$DESC" \
            -f target_url="${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"

  # ---------------------------------------------------------------------------
  # Cleanup - Stop all servers started by this workflow run
  # ---------------------------------------------------------------------------
  cleanup-server:
    needs: [e2e-tests]
    runs-on: self-hosted
    if: always()
    steps:
      - name: Stop Cypress Servers
        run: |
          echo "üõë Stopping webpack dev server for run_id: ${{ github.run_id }}..."

          PORT_INFO_DIR="/tmp/gha-ports"
          CURRENT_RUN_ID="${{ github.run_id }}"
          KILLED_COUNT=0

          # Find all port files owned by this run_id
          if [ -d "$PORT_INFO_DIR" ]; then
            for port_file in "$PORT_INFO_DIR"/port-*.run_id; do
              if [ -f "$port_file" ]; then
                PORT_OWNER_RUN_ID=$(cat "$port_file")
                if [ "$PORT_OWNER_RUN_ID" = "$CURRENT_RUN_ID" ]; then
                  # Extract port number from filename
                  PORT=$(basename "$port_file" | sed 's/port-\([0-9]*\)\.run_id/\1/')
                  PID_FILE="$PORT_INFO_DIR/port-${PORT}.pid"
                  
                  # Kill process if PID file exists
                  if [ -f "$PID_FILE" ]; then
                    PID=$(cat "$PID_FILE")
                    if ps -p "$PID" > /dev/null 2>&1; then
                      echo "üõë Killing process $PID on port $PORT (run_id: $CURRENT_RUN_ID)"
                      pkill -P "$PID" 2>/dev/null || true
                      kill "$PID" 2>/dev/null || true
                      KILLED_COUNT=$((KILLED_COUNT + 1))
                    fi
                  fi
                  
                  # Also kill any process on this port (double-check)
                  PORT_PID=$(lsof -ti:${PORT} 2>/dev/null | head -1)
                  if [ -n "$PORT_PID" ]; then
                    echo "üõë Killing process $PORT_PID on port $PORT"
                    pkill -P "$PORT_PID" 2>/dev/null || true
                    kill "$PORT_PID" 2>/dev/null || true
                  fi
                  
                  # Clean up orphaned Chrome processes
                  ALL_PORT_PIDS=$(lsof -ti:${PORT} 2>/dev/null || true)
                  if [ -n "$ALL_PORT_PIDS" ]; then
                    for port_pid in $ALL_PORT_PIDS; do
                      if ps -p "$port_pid" -o comm= 2>/dev/null | grep -qE "chrome|chromium"; then
                        echo "üõë Killing Chrome process $port_pid (using port $PORT)"
                        pkill -P "$port_pid" 2>/dev/null || true
                        kill "$port_pid" 2>/dev/null || true
                      fi
                    done
                  fi
                  
                  # Clean up port info files
                  rm -f "$port_file" "$PID_FILE"
                fi
              fi
            done
          fi

          # Clean up stale port files older than 24 hours
          find "$PORT_INFO_DIR" -name "*.run_id" -mtime +1 -delete 2>/dev/null || true
          find "$PORT_INFO_DIR" -name "*.pid" -mtime +1 -delete 2>/dev/null || true

          if [ $KILLED_COUNT -eq 0 ]; then
            echo "‚úÖ No processes found for run_id: $CURRENT_RUN_ID"
          else
            echo "‚úÖ Cleaned up $KILLED_COUNT process(es) for run_id: $CURRENT_RUN_ID"
          fi
